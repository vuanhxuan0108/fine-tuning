{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning full model distilgpt2","metadata":{}},{"cell_type":"markdown","source":"## Cài đặt các thư viện cần thiết","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets\n!pip install -q transformers\n!pip install -q trl\n!pip install -q -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:40:36.479143Z","iopub.execute_input":"2025-07-11T08:40:36.479397Z","iopub.status.idle":"2025-07-11T08:40:50.348055Z","shell.execute_reply.started":"2025-07-11T08:40:36.479373Z","shell.execute_reply":"2025-07-11T08:40:50.347248Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Tải lên dữ liệu imdb để thực hiện fine tuning model","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"imdb\", split=\"train[:1%]\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-07-11T08:40:50.348929Z","iopub.execute_input":"2025-07-11T08:40:50.349136Z","iopub.status.idle":"2025-07-11T08:40:53.473933Z","shell.execute_reply.started":"2025-07-11T08:40:50.349113Z","shell.execute_reply":"2025-07-11T08:40:53.473273Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:40:53.474648Z","iopub.execute_input":"2025-07-11T08:40:53.474977Z","iopub.status.idle":"2025-07-11T08:40:53.482737Z","shell.execute_reply.started":"2025-07-11T08:40:53.474958Z","shell.execute_reply":"2025-07-11T08:40:53.480044Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Hàm loại bỏ xuống dòng cho từng mẫu","metadata":{}},{"cell_type":"code","source":"def preprocessing(batch):\n    batch['text'] = [text.replace('\\n', '') for text in batch['text']]\n    return batch\n\ndataset = dataset.map(preprocessing, batched=True)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:40:53.484956Z","iopub.execute_input":"2025-07-11T08:40:53.485719Z","iopub.status.idle":"2025-07-11T08:40:53.504876Z","shell.execute_reply.started":"2025-07-11T08:40:53.485691Z","shell.execute_reply":"2025-07-11T08:40:53.504103Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:40:53.506288Z","iopub.execute_input":"2025-07-11T08:40:53.506642Z","iopub.status.idle":"2025-07-11T08:40:56.855927Z","shell.execute_reply.started":"2025-07-11T08:40:53.506610Z","shell.execute_reply":"2025-07-11T08:40:56.855327Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Khai báo mô hình, tải lên tokenizer tương ứng","metadata":{}},{"cell_type":"code","source":"model_name = \"distilgpt2\" # Tên mô hình tiến hành fine tuning\ntokenizer=AutoTokenizer.from_pretrained(model_name) # Tải lên tokenizer tương ứng với mô hình từ Hugging Face\nmodel = AutoModelForCausalLM.from_pretrained(model_name) # Tải lên mô hình distilgpt2, dùng cho việc sinh văn bản (AutoModelForCausalLM)\n\ntokenizer.pad_token=tokenizer.eos_token # Thực hiện tokenizer và pad_token = eos_token (từ cuối cùng trong văn bản)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:40:56.856657Z","iopub.execute_input":"2025-07-11T08:40:56.857018Z","iopub.status.idle":"2025-07-11T08:41:02.484530Z","shell.execute_reply.started":"2025-07-11T08:40:56.856999Z","shell.execute_reply":"2025-07-11T08:41:02.483690Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-07-11 08:40:59.588611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752223259.611560     221 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752223259.618309     221 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Thực hiện tokenizer và đệm","metadata":{}},{"cell_type":"code","source":"def tokenizing_function(examples):\n    tokenized = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128) # Chuyển văn bản thành các vector input_ids\n\n    tokenized['labels'] = tokenized['input_ids'].copy() # Gán cho label chính là vector đầu vào\n\n    return tokenized # trả về vector \n\ntokenized_data = dataset.map(tokenizing_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:02.485429Z","iopub.execute_input":"2025-07-11T08:41:02.486115Z","iopub.status.idle":"2025-07-11T08:41:02.522197Z","shell.execute_reply.started":"2025-07-11T08:41:02.486085Z","shell.execute_reply":"2025-07-11T08:41:02.521279Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:02.522934Z","iopub.execute_input":"2025-07-11T08:41:02.523130Z","iopub.status.idle":"2025-07-11T08:41:02.540104Z","shell.execute_reply.started":"2025-07-11T08:41:02.523113Z","shell.execute_reply":"2025-07-11T08:41:02.539322Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Khai báo các tham số cho quá trình huấn luyện","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments( # Định nghĩa các tham số trong quá trình huấn luyện\n    output_dir=\"./results\", # Thư mục lưu mô hình sau mỗi checkpoint\n    eval_strategy=\"epoch\", # Thực hiện đánh giá trên tập validation sau mỗi epoch\n    per_device_train_batch_size=4, # batch_size khi huấn luyện\n    per_device_eval_batch_size=4, # batch_size khi đánh giá trên tập validation\n    num_train_epochs=1, # Số lượng epoch để huấn luyện\n    logging_dir=\"./logs\", # Thư mục lưu log Tensorboard để xem lại các biểu đồ trong quá trình train\n    logging_steps=10,# Ghi log sau mỗi 10 step huấn luyện (hiển thị loss, learning rate...)\n    save_total_limit=1 # Chỉ lưu lại 1 checkpoint tại thời điểm tốt nhất\n)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:02.541094Z","iopub.execute_input":"2025-07-11T08:41:02.541669Z","iopub.status.idle":"2025-07-11T08:41:02.842288Z","shell.execute_reply.started":"2025-07-11T08:41:02.541641Z","shell.execute_reply":"2025-07-11T08:41:02.841559Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Chia tập dữ liệu train, validation","metadata":{}},{"cell_type":"code","source":"train_data = tokenized_data.shuffle().select(range(int(0.8 * len(tokenized_data))))\neval_data = tokenized_data.shuffle().select(range(int(0.8 * len(tokenized_data)), len(tokenized_data)))","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:02.843244Z","iopub.execute_input":"2025-07-11T08:41:02.843554Z","iopub.status.idle":"2025-07-11T08:41:02.870099Z","shell.execute_reply.started":"2025-07-11T08:41:02.843529Z","shell.execute_reply":"2025-07-11T08:41:02.869332Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Huấn luyện mô hình","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\ntrainer = Trainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = eval_data\n)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:02.871036Z","iopub.execute_input":"2025-07-11T08:41:02.871280Z","iopub.status.idle":"2025-07-11T08:41:03.770102Z","shell.execute_reply.started":"2025-07-11T08:41:02.871260Z","shell.execute_reply":"2025-07-11T08:41:03.769489Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:03.770921Z","iopub.execute_input":"2025-07-11T08:41:03.771152Z","iopub.status.idle":"2025-07-11T08:41:11.641493Z","shell.execute_reply.started":"2025-07-11T08:41:03.771134Z","shell.execute_reply":"2025-07-11T08:41:11.640691Z"},"trusted":true},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:06, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.037200</td>\n      <td>3.946259</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=25, training_loss=2.0487976837158204, metrics={'train_runtime': 7.4906, 'train_samples_per_second': 26.7, 'train_steps_per_second': 3.338, 'total_flos': 6532418764800.0, 'train_loss': 2.0487976837158204, 'epoch': 1.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Lưu lại model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./fine-tuned_model\")","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:11.644886Z","iopub.execute_input":"2025-07-11T08:41:11.645250Z","iopub.status.idle":"2025-07-11T08:41:12.751516Z","shell.execute_reply.started":"2025-07-11T08:41:11.645233Z","shell.execute_reply":"2025-07-11T08:41:12.750865Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Lưu lại tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./fine-tuned_model\")","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:12.752245Z","iopub.execute_input":"2025-07-11T08:41:12.752531Z","iopub.status.idle":"2025-07-11T08:41:12.804954Z","shell.execute_reply.started":"2025-07-11T08:41:12.752503Z","shell.execute_reply":"2025-07-11T08:41:12.804289Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('./fine-tuned_model/tokenizer_config.json',\n './fine-tuned_model/special_tokens_map.json',\n './fine-tuned_model/vocab.json',\n './fine-tuned_model/merges.txt',\n './fine-tuned_model/added_tokens.json',\n './fine-tuned_model/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Thực hiện 1 truy vấn với model fine tuning","metadata":{}},{"cell_type":"code","source":"prompt = \"Titanic\"\n\ninputs = tokenizer(prompt,  return_tensors=\"pt\").to(\"cuda\")\n\noutput = model.generate(inputs['input_ids'], max_length=128)\n\nprint(tokenizer.decode(output[0], skip_special_token=True))","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:12.805700Z","iopub.execute_input":"2025-07-11T08:41:12.805933Z","iopub.status.idle":"2025-07-11T08:41:13.513193Z","shell.execute_reply.started":"2025-07-11T08:41:12.805907Z","shell.execute_reply":"2025-07-11T08:41:13.512378Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Titanic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Fine tuning model TinyLlama-1.1B-Chat-v1.0 dùng kỹ thuật QLoRa","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, # Tự động tải mô hình dạng sinh văn bản\n    AutoTokenizer, # Tự động tải tokenizer tương ứng với mô hình\n    BitsAndBytesConfig, # cấu hình cho quantization\n    HfArgumentParser, # Chưa được dùng trong code\n    TrainingArguments, # Cấu hình cho quá trình huấn luyện (batch_size, epoch,...)\n    pipeline, # giúp tạo inference pipeline nhanh (chatbot, text generation...)\n    logging, # cấu hình logging khi train model\n)\nfrom peft import LoraConfig, PeftModel \n# LoraConfig: cấu hình cho fine-tuning LoRA (số rank, alpha, dropout...)\n# PeftModel: kết hợp mô hình gốc với trọng số đã fine-tune bằng LoRA\nfrom trl import SFTTrainer, SFTConfig\n# SFTTrainer: lớp trainer hỗ trợ huấn luyện Supervised Fine-Tuning (SFT), tích hợp với LoRA\n# SFTConfig: cấu hình cho SFTTrainer (batch size, sequence length, optimizer...)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:13.514142Z","iopub.execute_input":"2025-07-11T08:41:13.514748Z","iopub.status.idle":"2025-07-11T08:41:13.609727Z","shell.execute_reply.started":"2025-07-11T08:41:13.514723Z","shell.execute_reply":"2025-07-11T08:41:13.609087Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Các tham số và cấu hình cho quá trình train","metadata":{}},{"cell_type":"code","source":"model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Tên mô hình được fine tuning\ndataset_name = \"mlabonne/guanaco-llama2-1k\" # Tên dataset dùng cho quá trình fine tuning\nnew_model = \"TinyLlama-1.1B-Chat-finetune\" # Tên mô hình sau khi fine tuning\n\n\nlora_r = 64 # rank của ma trận low-rank\nlora_alpha = 16 # Hệ số scale\nlora_dropout = 0.1 # Tỷ lệ dropout để tránh hiện tượng overfitting\nuse_4bit = True # DÙng quantization 4-bit (tiết kiệm ram tối đa)\nbnb_4bit_compute_dtype = \"float16\" # data_type để tính toán\nbnb_4bit_quant_type = \"nf4\" # nf4 chính xác hơn int4\nuse_nested_quant = False # nested quant không dùng (ít được hỗ trợ)\n\n\n\noutput_dir = \"./results\" # Đường dẫn lưu mô hình sau fine tuning\nnum_train_epochs = 1 # Số lượng epoch để huấn luyện\nfp16 = False # tắt hỗ trợ half-precision\nbf16 = False # tắt hỗ trợ half-precision\nper_device_train_batch_size = 1 # Batch_size khi huấn luyện\nper_device_eval_batch_size = 1 # Batch_size khi validation\ngradient_accumulation_steps = 1 # 1, tức không cộng dồn gradient\ngradient_checkpointing = True # Tiết kiệm RAM bằng cách recompute forward\nmax_grad_norm = 0.3 # Gradient clipping để tránh exploding gradient\nlearning_rate = 2e-4 # Learning rate cơ bản cho LoRA\nweight_decay = 0.001 # Để tránh overfitting\noptim = \"paged_adamw_32bit\" # Bộ tối ưu hóa hỗ trợ 4bit training\nlr_scheduler_type = \"cosine\" # Dạng decay learning rate\nmax_steps = -1 # Sử dụng số epoch thay vì số bước\nwarmup_ratio = 0.03  # Tăng LR dần từ 0 → LR chính trong 3% bước đầu\ngroup_by_length = True # Nhóm mẫu theo độ dài → giảm padding\nsave_steps = 0 # Không save giữa chừng\nlogging_steps = 50 # Mỗi 50 step thì log 1 lần\nmax_seq_length = None # Sẽ dùng chiều dài tối đa của tokenizer\npacking = False # Không gộp nhiều sample thành 1 chuỗi dài\ndevice_map = {\"\": 0} # Chạy trên GPU 0","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:41:13.610460Z","iopub.execute_input":"2025-07-11T08:41:13.610731Z","iopub.status.idle":"2025-07-11T08:41:13.616752Z","shell.execute_reply.started":"2025-07-11T08:41:13.610705Z","shell.execute_reply":"2025-07-11T08:41:13.615969Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Load lên dataset","metadata":{}},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:41:13.617585Z","iopub.execute_input":"2025-07-11T08:41:13.617764Z","iopub.status.idle":"2025-07-11T08:41:14.543115Z","shell.execute_reply.started":"2025-07-11T08:41:13.617750Z","shell.execute_reply":"2025-07-11T08:41:14.542542Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Cấu hình cho quantization -> 4bit","metadata":{}},{"cell_type":"code","source":"# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:41:14.543813Z","iopub.execute_input":"2025-07-11T08:41:14.544017Z","iopub.status.idle":"2025-07-11T08:41:14.550508Z","shell.execute_reply.started":"2025-07-11T08:41:14.544001Z","shell.execute_reply":"2025-07-11T08:41:14.549927Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Load lên model và thực hiện quá trình quantization","metadata":{}},{"cell_type":"code","source":"# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:41:14.551297Z","iopub.execute_input":"2025-07-11T08:41:14.552096Z","iopub.status.idle":"2025-07-11T08:41:18.450969Z","shell.execute_reply.started":"2025-07-11T08:41:14.552078Z","shell.execute_reply":"2025-07-11T08:41:18.450096Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Load tokenizer tương ứng với mô hình","metadata":{}},{"cell_type":"code","source":"# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, model_max_length=2048)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:41:18.452015Z","iopub.execute_input":"2025-07-11T08:41:18.452334Z","iopub.status.idle":"2025-07-11T08:41:18.648387Z","shell.execute_reply.started":"2025-07-11T08:41:18.452307Z","shell.execute_reply":"2025-07-11T08:41:18.647776Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Cấu hình cho phần low rank","metadata":{}},{"cell_type":"code","source":"# Load LoRA configuration (cấu hình)\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:41:18.649167Z","iopub.execute_input":"2025-07-11T08:41:18.649394Z","iopub.status.idle":"2025-07-11T08:41:18.653599Z","shell.execute_reply.started":"2025-07-11T08:41:18.649376Z","shell.execute_reply":"2025-07-11T08:41:18.652881Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Khai báo các tham số và cấu hình cho quá trình huấn luyện","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    processing_class=tokenizer,\n    args= SFTConfig(\n        dataset_text_field=\"text\",\n        max_seq_length=max_seq_length,\n        output_dir=output_dir,\n        num_train_epochs=num_train_epochs,\n        per_device_train_batch_size=per_device_train_batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        optim=optim,\n        save_steps=save_steps,\n        logging_steps=logging_steps,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        fp16=fp16,\n        bf16=bf16,\n        max_grad_norm=max_grad_norm,\n        max_steps=max_steps,\n        warmup_ratio=warmup_ratio,\n        group_by_length=group_by_length,\n        lr_scheduler_type=lr_scheduler_type,\n        report_to=\"tensorboard\"\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:42:07.428017Z","iopub.execute_input":"2025-07-11T08:42:07.428317Z","iopub.status.idle":"2025-07-11T08:42:07.715425Z","shell.execute_reply.started":"2025-07-11T08:42:07.428294Z","shell.execute_reply":"2025-07-11T08:42:07.714796Z"}},"outputs":[{"name":"stderr","text":"average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Huấn luyện mô hình","metadata":{}},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:42:12.651427Z","iopub.execute_input":"2025-07-11T08:42:12.652252Z","iopub.status.idle":"2025-07-11T08:48:49.778570Z","shell.execute_reply.started":"2025-07-11T08:42:12.652225Z","shell.execute_reply":"2025-07-11T08:48:49.777683Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 06:34, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.927200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.833800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.789100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.790500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.778700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.820900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.847600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.846800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.844100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.831900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.8310468673706055, metrics={'train_runtime': 396.5146, 'train_samples_per_second': 2.522, 'train_steps_per_second': 1.261, 'total_flos': 2676813304922112.0, 'train_loss': 0.8310468673706055})"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# Lưu lại mô hình sau khi thực hiện fine tuning","metadata":{}},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:48:49.780183Z","iopub.execute_input":"2025-07-11T08:48:49.781013Z","iopub.status.idle":"2025-07-11T08:48:49.985379Z","shell.execute_reply.started":"2025-07-11T08:48:49.780984Z","shell.execute_reply":"2025-07-11T08:48:49.984803Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:55:18.058391Z","iopub.execute_input":"2025-07-11T08:55:18.059304Z","iopub.status.idle":"2025-07-11T08:55:24.924466Z","shell.execute_reply.started":"2025-07-11T08:55:18.059266Z","shell.execute_reply":"2025-07-11T08:55:24.923760Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<s>[INST] What is a large language model? [/INST] Large language models (LLMs) are models that can understand vast amounts of natural language. They are used for tasks such as machine translation, chatbots, and natural language processing (NLP).\n\nLLMs are trained on vast amounts of text data, including millions of sentences and paragraphs from a wide range of sources. These sources are collected through a variety of techniques, including online search, social media, and human-curated content.\n\nOne of the key advantages of LLMs is their ability to understand the structure and relationships between different words and phrases in natural language. This helps them to generate accurate translations, generate natural-sounding responses to questions, and generate meaningful responses to natural language prompts.\n\nLLMs are also highly accurate in some specific areas of language understanding, such as paraphrasing, summarizing, and generating responses to\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Empty VRAM\ndel model\ndel pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:48:53.289566Z","iopub.execute_input":"2025-07-11T08:48:53.289802Z","iopub.status.idle":"2025-07-11T08:48:54.048336Z","shell.execute_reply.started":"2025-07-11T08:48:53.289783Z","shell.execute_reply":"2025-07-11T08:48:54.047737Z"},"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"# Load lại model với fp16 và tokenizer","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:48:54.049092Z","iopub.execute_input":"2025-07-11T08:48:54.049330Z","iopub.status.idle":"2025-07-11T08:48:55.555624Z","shell.execute_reply.started":"2025-07-11T08:48:54.049311Z","shell.execute_reply":"2025-07-11T08:48:55.554933Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:48:55.556387Z","iopub.execute_input":"2025-07-11T08:48:55.556650Z","iopub.status.idle":"2025-07-11T08:48:55.560541Z","shell.execute_reply.started":"2025-07-11T08:48:55.556631Z","shell.execute_reply":"2025-07-11T08:48:55.559816Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Push lên Hugging Face","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")  # ⚠️ Không an toàn nếu chia sẻ notebook\n","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:48:55.561280Z","iopub.execute_input":"2025-07-11T08:48:55.561558Z","iopub.status.idle":"2025-07-11T08:48:55.632687Z","shell.execute_reply.started":"2025-07-11T08:48:55.561540Z","shell.execute_reply":"2025-07-11T08:48:55.632024Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# !huggingface-cli login\n\nmodel.push_to_hub(\"vuxxxan/TinyLlama-1.1B-Chat-finetune\", create_pr=True)\n\ntokenizer.push_to_hub(\"vuxxxan/TinyLlama-1.1B-Chat-finetune\",create_pr=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-11T08:48:55.633303Z","iopub.execute_input":"2025-07-11T08:48:55.633521Z","iopub.status.idle":"2025-07-11T08:49:24.415119Z","shell.execute_reply.started":"2025-07-11T08:48:55.633504Z","shell.execute_reply":"2025-07-11T08:49:24.414479Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3acadb88b340e88bf20d58bbd5863c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7e0edee3964681851025aa82d3d003"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/vuxxxan/TinyLlama-1.1B-Chat-finetune/commit/a401cf6e71d2f265b0033da1fb9f22f3c662c26d', commit_message='Upload tokenizer', commit_description='', oid='a401cf6e71d2f265b0033da1fb9f22f3c662c26d', pr_url='https://huggingface.co/vuxxxan/TinyLlama-1.1B-Chat-finetune/discussions/2', repo_url=RepoUrl('https://huggingface.co/vuxxxan/TinyLlama-1.1B-Chat-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='vuxxxan/TinyLlama-1.1B-Chat-finetune'), pr_revision='refs/pr/2', pr_num=2)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi = HfApi()\n\napi.upload_folder(\n    folder_path=\"TinyLlama-1.1B-Chat-finetune\",  # Thư mục chứa mô hình fine-tune\n    repo_id=\"vuxxxan/TinyLlama-1.1B-Chat-finetune\",  # Đúng với repo bạn đã tạo trên HF\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:49:24.415857Z","iopub.execute_input":"2025-07-11T08:49:24.416100Z","iopub.status.idle":"2025-07-11T08:49:45.700467Z","shell.execute_reply.started":"2025-07-11T08:49:24.416073Z","shell.execute_reply":"2025-07-11T08:49:45.699797Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5942ece5ef8641128f237c7b33d6fe77"}},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/vuxxxan/TinyLlama-1.1B-Chat-finetune/commit/4a38ed27b88991e5fffd8d861351bf481645254d', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4a38ed27b88991e5fffd8d861351bf481645254d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/vuxxxan/TinyLlama-1.1B-Chat-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='vuxxxan/TinyLlama-1.1B-Chat-finetune'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"vuxxxan/TinyLlama-1.1B-Chat-finetune\")\ntokenizer = AutoTokenizer.from_pretrained(\"vuxxxan/TinyLlama-1.1B-Chat-finetune\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:52:21.119206Z","iopub.execute_input":"2025-07-11T08:52:21.119870Z","iopub.status.idle":"2025-07-11T08:52:26.030185Z","shell.execute_reply.started":"2025-07-11T08:52:21.119837Z","shell.execute_reply":"2025-07-11T08:52:26.029304Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"698ed4aa84fe4de09ecec9d3e7381d38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"181810a16b1545ac81c78a63c5f7f2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/36.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40420dd55a944890ba0da395663d6224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/951 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8431be140b0741ac90038e1115254027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d91e5976fdf471cb0251fb6a7f723c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f1a116fb7c430d885822ee61e61d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe87f22878a741e69143cbea04d3a194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fc22ccca3e94087bcc9626ed5573644"}},"metadata":{}}],"execution_count":35}]}